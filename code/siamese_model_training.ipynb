{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7GUMPSQCeSz"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTbtmk91CgrI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from datasets import load_dataset\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Lambda, Dense, Dropout, Concatenate, Embedding\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import time\n",
        "import string\n",
        "import json\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pickle\n",
        "import spacy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0LAV89FDHCV"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNaiyVGXCipF"
      },
      "outputs": [],
      "source": [
        "swap = False\n",
        "number_of_train_samples = \"Full\"\n",
        "number_of_test_samples = \"Full\"\n",
        "epochs = 1\n",
        "train_batch_size = 32\n",
        "test_batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rY2j-wx5Clst"
      },
      "outputs": [],
      "source": [
        "def swap_items_at_random_indices(list1, list2):\n",
        "    assert len(list1) == len(list2), \"Lists must be of the same length\"\n",
        "    num_swaps = int(len(list1)/2)\n",
        "    indices = random.sample(range(len(list1)), num_swaps)\n",
        "\n",
        "    for i in indices:\n",
        "        list1[i], list2[i] = list2[i], list1[i]\n",
        "\n",
        "    return list1, list2\n",
        "\n",
        "snli_dataset = load_dataset(\"stanfordnlp/snli\")\n",
        "mnli_dataset = load_dataset(\"nyu-mll/multi_nli\")\n",
        "snli_train_dataset = snli_dataset[\"train\"]\n",
        "snli_test_dataset = snli_dataset[\"test\"]\n",
        "snli_train_dataset = snli_train_dataset.filter(lambda example: example[\"label\"] != -1)\n",
        "snli_test_dataset = snli_test_dataset.filter(lambda example: example[\"label\"] != -1)\n",
        "mnli_train_dataset = mnli_dataset[\"train\"]\n",
        "mnli_test_dataset = mnli_dataset[\"validation_matched\"]\n",
        "mnli_test_mismatch_dataset = mnli_dataset[\"validation_mismatched\"]\n",
        "mnli_train_dataset = mnli_train_dataset.filter(lambda example: example[\"label\"] != -1)\n",
        "mnli_test_dataset = mnli_test_dataset.filter(lambda example: example[\"label\"] != -1)\n",
        "mnli_test_mismatch_dataset = mnli_test_mismatch_dataset.filter(lambda example: example[\"label\"]!=-1)\n",
        "\n",
        "\n",
        "snli_train_premises = list(snli_train_dataset[\"premise\"])\n",
        "snli_train_hypothesis = list(snli_train_dataset[\"hypothesis\"])\n",
        "snli_train_labels = list(snli_train_dataset[\"label\"])\n",
        "\n",
        "snli_test_premises = list(snli_test_dataset[\"premise\"])\n",
        "snli_test_hypothesis = list(snli_test_dataset[\"hypothesis\"])\n",
        "snli_test_labels = list(snli_test_dataset[\"label\"])\n",
        "\n",
        "mnli_train_premises = list(mnli_train_dataset[\"premise\"])\n",
        "mnli_train_hypothesis = list(mnli_train_dataset[\"hypothesis\"])\n",
        "mnli_train_labels = list(mnli_train_dataset[\"label\"])\n",
        "\n",
        "mnli_test_premises = list(mnli_test_dataset[\"premise\"])\n",
        "mnli_test_hypothesis = list(mnli_test_dataset[\"hypothesis\"])\n",
        "mnli_test_labels = list(mnli_test_dataset[\"label\"])\n",
        "\n",
        "mnli_test_mismatched_premises = list(mnli_test_mismatch_dataset[\"premise\"])\n",
        "mnli_test_mismatched_hypothesis = list(mnli_test_mismatch_dataset[\"hypothesis\"])\n",
        "mnli_test_mismatched_labels = list(mnli_test_mismatch_dataset[\"label\"])\n",
        "\n",
        "\n",
        "all_train_premises =  snli_train_premises + mnli_train_premises\n",
        "all_train_hypothesis =  snli_train_hypothesis + mnli_train_hypothesis\n",
        "if swap:\n",
        "  all_train_premises, all_train_hypothesis = swap_items_at_random_indices(all_train_premises, all_train_hypothesis)\n",
        "\n",
        "all_test_premises =  mnli_test_premises#snli_test_premises #+ mnli_test_premises #+mnli_test_mismatched_premises # + mnli_test_premises\n",
        "all_test_hypothesis = mnli_test_hypothesis# snli_test_hypothesis #+ mnli_test_hypothesis # mnli_test_mismatched_hypothesis #+ mnli_test_hypothesis\n",
        "if swap:\n",
        "  all_test_premises, all_test_hypothesis = swap_items_at_random_indices(all_test_premises, all_test_hypothesis)\n",
        "\n",
        "all_train_labels =  snli_train_labels + mnli_train_labels\n",
        "all_test_labels =  mnli_test_labels# snli_test_labels # + mnli_test_labels # mnli_test_mismatched_labels #+ mnli_test_labels\n",
        "\n",
        "\n",
        "if number_of_train_samples != \"Full\":\n",
        "    train_indices = random.sample(range(len(all_train_premises)), number_of_train_samples)\n",
        "    all_train_premises = [all_train_premises[i] for i in train_indices]\n",
        "    all_train_hypothesis = [all_train_hypothesis[i] for i in train_indices]\n",
        "    all_train_labels = [all_train_labels[i] for i in train_indices]\n",
        "\n",
        "if number_of_test_samples != \"Full\":\n",
        "    test_indices = random.sample(range(len(all_test_premises)), number_of_test_samples)\n",
        "    all_test_premises = [all_test_premises[i] for i in test_indices]\n",
        "    all_test_hypothesis = [all_test_hypothesis[i] for i in test_indices]\n",
        "    all_test_labels = [all_test_labels[i] for i in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp3P4aRqCqYi"
      },
      "outputs": [],
      "source": [
        "# 1. Dataset class\n",
        "class SNLIDataset(Dataset):\n",
        "    def __init__(self, premises, hypotheses, labels, tokenizer, max_length=128):\n",
        "        self.premises = premises\n",
        "        self.hypotheses = hypotheses\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        enc_a = self.tokenizer(\n",
        "            self.premises[idx],\n",
        "            padding='max_length', truncation=True,\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        enc_b = self.tokenizer(\n",
        "            self.hypotheses[idx],\n",
        "            padding='max_length', truncation=True,\n",
        "            max_length=self.max_length, return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids_a': enc_a['input_ids'].squeeze(0),\n",
        "            'attention_mask_a': enc_a['attention_mask'].squeeze(0),\n",
        "            'input_ids_b': enc_b['input_ids'].squeeze(0),\n",
        "            'attention_mask_b': enc_b['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# 2. Siamese BERT with MLP\n",
        "class SiameseBertClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model='bert-base-uncased', num_labels=3):\n",
        "        super(SiameseBertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
        "        for param in self.bert.parameters():\n",
        "          param.requires_grad = False\n",
        "        for i in range(8,12):\n",
        "          for param in self.bert.encoder.layer[i].parameters():\n",
        "              param.requires_grad = True\n",
        "        hidden_size = self.bert.config.hidden_size  # 768\n",
        "        combined_dim = hidden_size * 4  # u, v, |u-v|, u*v\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids_a, attention_mask_a, input_ids_b, attention_mask_b):\n",
        "        u = self.bert(input_ids=input_ids_a, attention_mask=attention_mask_a).last_hidden_state[:, 0, :]\n",
        "        v = self.bert(input_ids=input_ids_b, attention_mask=attention_mask_b).last_hidden_state[:, 0, :]\n",
        "\n",
        "        abs_diff = torch.abs(u - v)\n",
        "        elem_mult = u * v\n",
        "        combined = torch.cat([u, v, abs_diff, elem_mult], dim=1)\n",
        "\n",
        "        return self.classifier(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkDaQsVNC1Wp"
      },
      "outputs": [],
      "source": [
        "# 3. Evaluation function\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids_a = batch['input_ids_a'].to(device)\n",
        "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
        "            input_ids_b = batch['input_ids_b'].to(device)\n",
        "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            logits = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            for pred in preds:\n",
        "              all_preds.append(pred)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    return all_preds\n",
        "\n",
        "# 4. Training function\n",
        "def train(model, train_loader, test_loader, optimizer, device, epochs=3):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        print(f\"Epoch {epoch + 1}\")\n",
        "        loop = tqdm(train_loader, leave=True)\n",
        "        for batch in loop:\n",
        "            input_ids_a = batch['input_ids_a'].to(device)\n",
        "            attention_mask_a = batch['attention_mask_a'].to(device)\n",
        "            input_ids_b = batch['input_ids_b'].to(device)\n",
        "            attention_mask_b = batch['attention_mask_b'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids_a, attention_mask_a, input_ids_b, attention_mask_b)\n",
        "            loss = loss_fn(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(\"Test\")\n",
        "        evaluate(model, test_loader, device)\n",
        "        print(\"Train\")\n",
        "        evaluate(model, train_loader, device)\n",
        "\n",
        "\n",
        "# 5. Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    #tokenizer = BertTokenizer.from_pretrained(\"roberta-large\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Dummy data for demonstration\n",
        "    train_premises = all_train_premises\n",
        "    train_hypotheses = all_train_hypothesis\n",
        "    train_labels = all_train_labels  # entailment=0, neutral=1, contradiction=2\n",
        "\n",
        "    test_premises = all_test_premises\n",
        "    test_hypotheses = all_test_hypothesis\n",
        "    test_labels = all_test_labels  # entailment, contradiction\n",
        "\n",
        "    # # Dataset and DataLoader\n",
        "    train_dataset = SNLIDataset(train_premises, train_hypotheses, train_labels, tokenizer)\n",
        "    test_dataset = SNLIDataset(test_premises, test_hypotheses, test_labels, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=test_batch_size, shuffle=False)\n",
        "\n",
        "    # Model + Optimizer\n",
        "    model = SiameseBertClassifier(num_labels=3).to(device)\n",
        "    optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
        "\n",
        "    # Train\n",
        "    train(model, train_loader, test_loader, optimizer, device, epochs=epochs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oDFg26mDJyQ"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "of6oIDzIDMD9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_excel(\"./Candidate_sentence_pairs.xlsx\", sheet_name=0)\n",
        "all_i = data[\"sentence_i\"].to_list()\n",
        "all_j = data[\"sentence_j\"].to_list()\n",
        "all_i = all_premise\n",
        "all_j = all_hypothesis\n",
        "all_i_embedding = []\n",
        "all_j_embedding = []\n",
        "batch_size = 2\n",
        "for i in tqdm(range(0,len(all_i), batch_size)):\n",
        "  selected_sentences_i = all_i[i:i+batch_size]\n",
        "  selected_sentences_j = all_j[i:i+batch_size]\n",
        "  selected_tokens_i = tokenizer(selected_sentences_i, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "  selected_tokens_j = tokenizer(selected_sentences_j, return_tensors=\"pt\", truncation=True, padding=True, max_length=128).to(device)\n",
        "  with torch.no_grad():\n",
        "    outputs_i = model.bert(**selected_tokens_i)\n",
        "    embeddings = outputs_i.last_hidden_state[:, 0, :]\n",
        "    all_i_embedding.append(embeddings.cpu())\n",
        "  with torch.no_grad():\n",
        "    outputs_j = model.bert(**selected_tokens_j)\n",
        "    embeddings = outputs_j.last_hidden_state[:, 0, :]\n",
        "    all_j_embedding.append(embeddings.cpu())\n",
        "all_i_embedding = torch.cat(all_i_embedding, dim=0)\n",
        "all_j_embedding = torch.cat(all_j_embedding, dim=0)\n",
        "\n",
        "all_j_embedding = all_j_embedding.to(device)\n",
        "all_i_embedding = all_i_embedding.to(device)\n",
        "\n",
        "abs_diff = torch.abs(all_i_embedding - all_j_embedding)\n",
        "elem_mult = (all_i_embedding * all_j_embedding).to(device)\n",
        "combined = torch.cat([all_i_embedding,all_j_embedding, abs_diff, elem_mult], dim=1).to(device)\n",
        "logits = model.classifier(combined)\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "all_probs = [probs]\n",
        "\n",
        "\n",
        "logits = probs.argmax(dim=1).cpu()\n",
        "to_label = {0:\"entailment\", 1:\"neutral\", 2:\"contradiction\"}\n",
        "labels = []\n",
        "for logit in logits:\n",
        "  #labels.append(to_label[logit.item()])\n",
        "  labels.append(logit.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxRFyVQQDg69"
      },
      "outputs": [],
      "source": [
        "data[\"label\"] = labels\n",
        "data.to_csv(\"edited_data_with_label.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
